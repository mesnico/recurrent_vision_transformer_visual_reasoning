{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/nicola/Data/Workspace/CORnet-SamedifferentTask\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os\n",
    "from transformer import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np\n",
    "from PIL.ImageOps import invert\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from matplotlib import rc\n",
    "\n",
    "# equivalent to rcParams['animation.html'] = 'html5'\n",
    "rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRTVal(object):\n",
    "\n",
    "    def __init__(self, model, dataset_path, angle=None, resize=False, normalize=False, image_set='val'):\n",
    "        self.resize = resize\n",
    "        self.normalize = normalize\n",
    "        self.angle = angle\n",
    "        self.image_set = image_set\n",
    "        self.dataset_path = dataset_path\n",
    "        self.name = image_set\n",
    "        self.model = model\n",
    "        self.data_loader = self.data()\n",
    "        self.shape = (self.model.vit.patch_res, self.model.vit.patch_res)\n",
    "\n",
    "    def data(self):\n",
    "        transforms = []\n",
    "        '''if self.angle is not None:\n",
    "            transforms.append(torchvision.transforms.Pad(80, fill=(255, 255, 255)))\n",
    "            transforms.append(torchvision.transforms.Lambda(\n",
    "                lambda img: torchvision.transforms.functional.rotate(img, self.angle, resample=Image.BILINEAR)\n",
    "            ))\n",
    "            transforms.append(torchvision.transforms.CenterCrop(192))'''\n",
    "        transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if self.dataset_path is not None:\n",
    "            val_dataset = self.dataset_path\n",
    "            print('Using validation from {}'.format(val_dataset))\n",
    "        else:\n",
    "            val_dataset = FLAGS.data_path\n",
    "\n",
    "        dataset = torchvision.datasets.ImageFolder(\n",
    "            os.path.join(val_dataset, self.image_set),\n",
    "            transforms)\n",
    "        data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                  batch_size=1,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            #inp = inp.cuda()\n",
    "            attn = self.model.get_last_selfattention(inp)\n",
    "            \n",
    "        return attn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv strides: [1, 2, 2, 2]; Num patches: 16 x 16\n",
      "Conv Model: <class 'transformer.vit.EquivariantConvModel'>\n",
      "Vit Depth: 0; U Transf Depth: 9\n",
      "Using validation from /media/nicola/SSD/Datasets/svrt-HEAD-d34ac2b/results_problem_5\n",
      "(tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         ...,\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         ...,\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         ...,\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]), 1)\n",
      "Self-attention shape: torch.Size([1, 4, 257, 257])\n",
      "tensor([True, True, True, True])\n",
      "Reshaped self-attention: torch.Size([16, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "checkpoint_path = \"runs/from_scratch_uncertainty/problem_5/econvviut-hires-medium_adam_lr0.0001_28000-training-set_depth0_udepth9_dropout_data-augmentation_multiloss_epochs160/best_checkpoint.pth.tar\"\n",
    "dataset_path = \"/media/nicola/SSD/Datasets/svrt-HEAD-d34ac2b/results_problem_5\"\n",
    "img_id = 56000\n",
    "\n",
    "model_chkp = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model = transformer_econvviut_hires_multiloss_medium(depth=0, u_depth=9)\n",
    "# print(model_chkp['state_dict'].keys())\n",
    "model.load_state_dict(model_chkp['state_dict'], strict=False)\n",
    "\n",
    "# Construct the main class and compute attention\n",
    "svrt_model = SVRTVal(model, dataset_path)\n",
    "sample = svrt_model.data_loader.dataset[img_id]\n",
    "print(sample)\n",
    "sample = sample[0].unsqueeze(0) # emulate batch size 1\n",
    "sattn = svrt_model(sample)[0]\n",
    "\n",
    "print(\"Self-attention shape:\", sattn.shape)\n",
    "\n",
    "shape = svrt_model.shape\n",
    "# average over heads\n",
    "interesting_head = (sattn[0, :, 0, 0].view(-1, 1, 1) != sattn[0, :, :, :]).any(1).any(1)\n",
    "print(interesting_head)\n",
    "sattn = sattn.mean(dim=1)\n",
    "\n",
    "\n",
    "# reshape\n",
    "sattn = sattn[0, 1:, 1:].reshape(shape + shape)\n",
    "print(\"Reshaped self-attention:\", sattn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sattn[4, 4, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling factor for the CNN\n",
    "fact = 128 // 8\n",
    "\n",
    "# let's select 4 reference points for visualization\n",
    "# idxs = [(64, 64), (64, 64), (64, 64), (64, 64),]\n",
    "idxs = [(35, 80)] * 4\n",
    "\n",
    "# here we create the canvas\n",
    "fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
    "# and we add one plot per reference point\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "axs = [\n",
    "    fig.add_subplot(gs[0, 0]),\n",
    "    fig.add_subplot(gs[1, 0]),\n",
    "    fig.add_subplot(gs[0, -1]),\n",
    "    fig.add_subplot(gs[1, -1]),\n",
    "]\n",
    "\n",
    "# for each one of the reference points, let's plot the self-attention\n",
    "# for that point\n",
    "for idx_o, ax in zip(idxs, axs):\n",
    "    idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
    "    print(idx)\n",
    "    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'self-attention{idx_o}')\n",
    "\n",
    "# and now let's add the central image, with the reference points as red circles\n",
    "fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
    "im = sample.squeeze(0).permute(1, 2, 0).numpy()\n",
    "fcenter_ax.imshow(im)\n",
    "for (y, x) in idxs:\n",
    "    scale = 1 # im.height / img.shape[-2]\n",
    "    x = ((x // fact) + 0.5) * fact\n",
    "    y = ((y // fact) + 0.5) * fact\n",
    "    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "    fcenter_ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        self.url = \"\"\n",
    "        self.cur_url = None\n",
    "        self.pil_img = None\n",
    "        self.tensor_img = None\n",
    "\n",
    "        self.conv_features = None\n",
    "        self.enc_attn_weights = None\n",
    "        self.dec_attn_weights = None\n",
    "\n",
    "        self.setup_widgets()\n",
    "\n",
    "    def setup_widgets(self):\n",
    "        self.sliders = [\n",
    "            widgets.Text(\n",
    "                value='14',\n",
    "                placeholder='Type something',\n",
    "                description='Img ID:',\n",
    "                disabled=False,\n",
    "                continuous_update=False,\n",
    "                layout=widgets.Layout(width='100%')\n",
    "            ),\n",
    "            widgets.FloatSlider(min=0, max=0.99,\n",
    "                        step=0.02, description='X coordinate', value=0.72,\n",
    "                        continuous_update=False,\n",
    "                        layout=widgets.Layout(width='50%')\n",
    "                        ),\n",
    "            widgets.FloatSlider(min=0, max=0.99,\n",
    "                        step=0.02, description='Y coordinate', value=0.40,\n",
    "                        continuous_update=False,\n",
    "                        layout=widgets.Layout(width='50%')),\n",
    "            widgets.Checkbox(\n",
    "              value=False,\n",
    "              description='Direction of self attention',\n",
    "              disabled=False,\n",
    "              indent=False,\n",
    "              layout=widgets.Layout(width='50%'),\n",
    "          ),\n",
    "            widgets.Checkbox(\n",
    "              value=False,\n",
    "              description='Show red dot in attention',\n",
    "              disabled=False,\n",
    "              indent=False,\n",
    "              layout=widgets.Layout(width='50%'),\n",
    "          )\n",
    "        ]\n",
    "        self.o = widgets.Output()\n",
    "\n",
    "    def compute_features(self, img):\n",
    "        model = self.model\n",
    "        # propagate through the model\n",
    "        img = img.unsqueeze(0) # emulate batch size 1\n",
    "        sattns = model(img)\n",
    "\n",
    "        p_sattns = []\n",
    "        for sattn in sattns:\n",
    "            shape = self.model.shape\n",
    "            # average over heads\n",
    "            # interesting_head = (sattn[0, :, 0, 0].view(-1, 1, 1) != sattn[0, :, :, :]).any(1).any(1)\n",
    "            # print(interesting_head)\n",
    "            sattn = sattn.mean(dim=1)\n",
    "\n",
    "            # reshape\n",
    "            sattn = sattn[0, 1:, 1:].reshape(shape + shape)\n",
    "            # print(\"Reshaped self-attention:\", sattn.shape)\n",
    "            \n",
    "            p_sattns.append(sattn)\n",
    "            \n",
    "        self.enc_attn_weights = p_sattns[-1]\n",
    "        self.enc_attns_weights = p_sattns\n",
    "    \n",
    "    def compute_on_image(self, url):\n",
    "        if url != self.url:\n",
    "            self.url = url\n",
    "            img_id = int(url)\n",
    "            sample = self.model.data_loader.dataset[img_id]\n",
    "            self.tensor_img = sample[0]\n",
    "            out_image = self.tensor_img * torch.Tensor([0.229, 0.224, 0.225]).view(-1, 1, 1) + torch.Tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "            print(self.tensor_img.shape)\n",
    "            self.pil_img = Image.fromarray((out_image.permute(1, 2, 0).numpy() * 255).astype(np.uint8))\n",
    "            self.compute_features(self.tensor_img)\n",
    "    \n",
    "    def update_chart(self, change):\n",
    "        with self.o:\n",
    "            clear_output()\n",
    "\n",
    "            # j and i are the x and y coordinates of where to look at\n",
    "            # sattn_dir is which direction to consider in the self-attention matrix\n",
    "            # sattn_dot displays a red dot or not in the self-attention map\n",
    "            url, j, i, sattn_dir, sattn_dot = [s.value for s in self.sliders]\n",
    "\n",
    "            fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(9, 4))\n",
    "            self.compute_on_image(url)\n",
    "\n",
    "            # convert reference point to absolute coordinates\n",
    "            j = int(j * self.tensor_img.shape[-1])\n",
    "            i = int(i * self.tensor_img.shape[-2])\n",
    "\n",
    "            # how much was the original image upsampled before feeding it to the model\n",
    "            scale = self.pil_img.height / self.tensor_img.shape[-2]\n",
    "\n",
    "            # compute the downsampling factor for the model\n",
    "            # it should be 32 for standard DETR and 16 for DC5\n",
    "            sattn = self.enc_attn_weights\n",
    "            fact = 2 ** round(math.log2(self.tensor_img.shape[-1] / sattn.shape[-1]))\n",
    "\n",
    "            # round the position at the downsampling factor\n",
    "            x = ((j // fact) + 0.5) * fact\n",
    "            y = ((i // fact) + 0.5) * fact\n",
    "\n",
    "            axs[0].imshow(self.pil_img)\n",
    "            axs[0].axis('off')\n",
    "            axs[0].add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "\n",
    "            idx = (i // fact, j // fact)\n",
    "            \n",
    "            if sattn_dir:\n",
    "                sattn_map = sattn[idx[0], idx[1], ...]\n",
    "            else:\n",
    "                sattn_map = sattn[..., idx[0], idx[1]]\n",
    "            \n",
    "            # axs[1].imshow(sattn_map, cmap='cividis', interpolation='nearest')\n",
    "            #if sattn_dot:\n",
    "                #axs[1].add_patch(plt.Circle((idx[1],idx[0]), 1, color='r'))\n",
    "            #axs[1].axis('off')\n",
    "            #axs[1].set_title(f'self-attention{(i, j)}')\n",
    "            \n",
    "            xmin, xmax, ymin, ymax = (0, self.pil_img.width, 0, self.pil_img.height)\n",
    "            axs[1].imshow(sattn_map, cmap='cividis', interpolation='nearest', extent=(xmin, xmax, ymin, ymax))\n",
    "            axs[1].imshow(invert(self.pil_img), alpha=0.3, extent=(xmin, xmax, ymin, ymax))\n",
    "            if sattn_dot:\n",
    "                axs[1].add_patch(plt.Circle((idx[1],idx[0]), 1, color='r'))\n",
    "            axs[1].set_title(f'self-attention{(i, j)}')\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            fig_anim, axs_anim = plt.subplots(figsize=(9, 4))\n",
    "            \n",
    "            def update(k):\n",
    "                axs_anim.axis('off')\n",
    "                sattn = self.enc_attns_weights[k]\n",
    "                if sattn_dir:\n",
    "                    sattn_map = sattn[idx[0], idx[1], ...]\n",
    "                else:\n",
    "                    sattn_map = sattn[..., idx[0], idx[1]]\n",
    "                axs_anim.set_title(f'self-attention{(i, j)}-timestep{k}')\n",
    "                axs_anim.imshow(sattn_map, cmap='cividis', interpolation='nearest', extent=(xmin, xmax, ymin, ymax))\n",
    "                axs_anim.imshow(invert(self.pil_img), alpha=0.3, extent=(xmin, xmax, ymin, ymax))\n",
    "                if sattn_dot:\n",
    "                    axs_anim.add_patch(plt.Circle((idx[1],idx[0]), 1, color='r'))\n",
    "                    \n",
    "            anim = FuncAnimation(fig_anim, update, frames=np.arange(0, 10), interval=500)\n",
    "            \n",
    "            plt.show()\n",
    "            # anim.save('line.gif', dpi=80, writer='imagemagick')\n",
    "            \n",
    "            # plots the attention over time-step in a single figure\n",
    "            fig, axs = plt.subplots(ncols=6, nrows=1, figsize=(9, 4))\n",
    "            axs[0].imshow(self.pil_img)\n",
    "            axs[0].axis('off')\n",
    "            axs[0].add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "            \n",
    "            def draw(k):\n",
    "                axs[k+1].axis('off')\n",
    "                sattn = self.enc_attns_weights[k]\n",
    "                if sattn_dir:\n",
    "                    sattn_map = sattn[idx[0], idx[1], ...]\n",
    "                else:\n",
    "                    sattn_map = sattn[..., idx[0], idx[1]]\n",
    "                axs[k+1].set_title(f't={k}')\n",
    "                axs[k+1].imshow(sattn_map, cmap='cividis', interpolation='nearest', extent=(xmin, xmax, ymin, ymax))\n",
    "                axs[k+1].imshow(invert(self.pil_img), alpha=0.3, extent=(xmin, xmax, ymin, ymax))\n",
    "#                 if sattn_dot:\n",
    "#                     axs[k+1].add_patch(plt.Circle((idx[1],idx[0]), 1, color='r'))\n",
    "            \n",
    "            for k in range(0, 5):\n",
    "                draw(k)\n",
    "                \n",
    "            plt.show()\n",
    "        \n",
    "    def run(self):\n",
    "      for s in self.sliders:\n",
    "          s.observe(self.update_chart, 'value')\n",
    "      self.update_chart(None)\n",
    "      url, x, y, d, sattn_d = self.sliders\n",
    "      res = widgets.VBox(\n",
    "      [\n",
    "          url,\n",
    "          widgets.HBox([x, y]),\n",
    "          widgets.HBox([d, sattn_d]),\n",
    "          self.o\n",
    "      ])\n",
    "      return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaa23402d4f4c10aaaae681f48c1bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='14', continuous_update=False, description='Img ID:', layout=Layout(width='100%'), p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = AttentionVisualizer(svrt_model)\n",
    "w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samedifferent",
   "language": "python",
   "name": "samedifferent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}